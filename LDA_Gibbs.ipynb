{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using Gibb's sampling\n",
    "\n",
    "Op basis van uitleg in http://u.cs.biu.ac.il/~89-680/darling-lda.pdf\n",
    "\n",
    "Using Gensim functionality\n",
    "\n",
    "Dus we mogen ervan uitgaan dat we een dictionary hebben en bow vectors voor de documenten.\n",
    "Het een en ander aan counts komt dan uit gensim. Maar hoe?\n",
    "\n",
    "Wat hebben we nodig?\n",
    "\n",
    "* The number of words assigned to topic k in document d: $$n_{d;k}$$\n",
    "* The number of times word w is assigned to topic k: $$n_{k;w}$$\n",
    "* The total number of times any word is assigned to topic k: $$n_k$$\n",
    "* Array z which will contain the current topic assignment for each of the N words in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate documents\n",
    "import numpy as np\n",
    "\n",
    "length = 100\n",
    "num_topics = 3\n",
    "vocabulary = np.array(['zon', 'ijs', 'strand', 'vanille', 'chocola', 'broccoli', 'wortel'])\n",
    "\n",
    "real_theta = np.array([[0.5, 0.0, 0.5],\n",
    "                       [0.3, 0.3, 0.4],\n",
    "                       [0.1, 0.8, 0.1],\n",
    "                       [0.7, 0.3, 0.0],\n",
    "                       [0.4, 0.4, 0.2]])\n",
    "real_phi = np.array([[0.4, 0.2, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
    "                     [0.0, 0.3, 0.0, 0.35, 0.35, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 ['wortel', 'broccoli', 'strand', 'broccoli', 'strand', 'broccoli', 'zon', 'broccoli', 'ijs', 'broccoli', 'broccoli', 'strand', 'zon', 'wortel', 'zon', 'wortel', 'wortel', 'zon', 'ijs', 'wortel', 'broccoli', 'ijs', 'ijs', 'wortel', 'strand', 'wortel', 'strand', 'strand', 'broccoli', 'zon', 'broccoli', 'broccoli', 'wortel', 'strand', 'wortel', 'zon', 'strand', 'zon', 'zon', 'ijs', 'zon', 'zon', 'ijs', 'broccoli', 'wortel', 'zon', 'wortel', 'wortel', 'wortel', 'broccoli', 'ijs', 'broccoli', 'zon', 'strand', 'broccoli', 'broccoli', 'broccoli', 'ijs', 'broccoli', 'ijs', 'wortel', 'wortel', 'zon', 'zon', 'wortel', 'strand', 'strand', 'wortel', 'zon', 'wortel', 'strand', 'wortel', 'broccoli', 'wortel', 'ijs', 'broccoli', 'wortel', 'wortel', 'broccoli', 'strand', 'zon', 'wortel', 'wortel', 'zon', 'strand', 'wortel', 'zon', 'strand', 'wortel', 'wortel', 'ijs', 'broccoli', 'broccoli', 'zon', 'wortel', 'zon', 'ijs', 'broccoli', 'strand', 'zon']\n",
      "document 2 ['wortel', 'vanille', 'ijs', 'wortel', 'chocola', 'wortel', 'broccoli', 'broccoli', 'strand', 'vanille', 'ijs', 'broccoli', 'ijs', 'ijs', 'zon', 'strand', 'ijs', 'vanille', 'vanille', 'chocola', 'wortel', 'broccoli', 'zon', 'wortel', 'zon', 'ijs', 'wortel', 'zon', 'broccoli', 'chocola', 'broccoli', 'chocola', 'broccoli', 'vanille', 'ijs', 'wortel', 'zon', 'wortel', 'zon', 'chocola', 'broccoli', 'vanille', 'wortel', 'ijs', 'strand', 'broccoli', 'zon', 'vanille', 'wortel', 'chocola', 'zon', 'wortel', 'broccoli', 'chocola', 'chocola', 'zon', 'broccoli', 'ijs', 'strand', 'ijs', 'wortel', 'vanille', 'strand', 'wortel', 'broccoli', 'zon', 'wortel', 'wortel', 'strand', 'wortel', 'chocola', 'chocola', 'wortel', 'strand', 'chocola', 'wortel', 'wortel', 'vanille', 'strand', 'vanille', 'wortel', 'zon', 'ijs', 'chocola', 'zon', 'chocola', 'chocola', 'wortel', 'ijs', 'broccoli', 'wortel', 'chocola', 'ijs', 'wortel', 'wortel', 'broccoli', 'broccoli', 'chocola', 'chocola', 'strand']\n",
      "document 3 ['chocola', 'chocola', 'vanille', 'ijs', 'ijs', 'ijs', 'zon', 'vanille', 'broccoli', 'wortel', 'ijs', 'chocola', 'vanille', 'zon', 'chocola', 'chocola', 'vanille', 'vanille', 'wortel', 'vanille', 'chocola', 'ijs', 'ijs', 'chocola', 'zon', 'vanille', 'chocola', 'vanille', 'vanille', 'ijs', 'chocola', 'chocola', 'vanille', 'ijs', 'ijs', 'ijs', 'ijs', 'ijs', 'vanille', 'vanille', 'ijs', 'vanille', 'chocola', 'vanille', 'zon', 'ijs', 'chocola', 'ijs', 'vanille', 'strand', 'chocola', 'vanille', 'chocola', 'zon', 'chocola', 'vanille', 'vanille', 'chocola', 'broccoli', 'ijs', 'ijs', 'vanille', 'vanille', 'vanille', 'zon', 'vanille', 'ijs', 'zon', 'vanille', 'chocola', 'vanille', 'vanille', 'ijs', 'vanille', 'chocola', 'vanille', 'chocola', 'vanille', 'ijs', 'chocola', 'vanille', 'vanille', 'wortel', 'chocola', 'ijs', 'wortel', 'ijs', 'ijs', 'chocola', 'vanille', 'ijs', 'zon', 'vanille', 'ijs', 'vanille', 'chocola', 'vanille', 'wortel', 'vanille', 'vanille']\n",
      "document 4 ['strand', 'zon', 'zon', 'strand', 'strand', 'strand', 'vanille', 'strand', 'vanille', 'zon', 'strand', 'vanille', 'ijs', 'chocola', 'chocola', 'zon', 'strand', 'vanille', 'strand', 'vanille', 'ijs', 'zon', 'chocola', 'ijs', 'zon', 'chocola', 'zon', 'vanille', 'zon', 'strand', 'ijs', 'chocola', 'zon', 'chocola', 'zon', 'zon', 'zon', 'strand', 'chocola', 'strand', 'zon', 'zon', 'strand', 'ijs', 'ijs', 'zon', 'chocola', 'zon', 'zon', 'zon', 'strand', 'vanille', 'chocola', 'zon', 'zon', 'zon', 'zon', 'ijs', 'zon', 'ijs', 'ijs', 'ijs', 'ijs', 'zon', 'ijs', 'zon', 'zon', 'chocola', 'zon', 'ijs', 'zon', 'zon', 'strand', 'ijs', 'zon', 'vanille', 'ijs', 'zon', 'vanille', 'ijs', 'zon', 'ijs', 'chocola', 'ijs', 'zon', 'ijs', 'ijs', 'ijs', 'zon', 'vanille', 'ijs', 'strand', 'chocola', 'ijs', 'ijs', 'ijs', 'vanille', 'zon', 'ijs', 'strand']\n",
      "document 5 ['zon', 'chocola', 'broccoli', 'zon', 'strand', 'broccoli', 'strand', 'ijs', 'ijs', 'zon', 'zon', 'strand', 'chocola', 'zon', 'chocola', 'ijs', 'strand', 'chocola', 'broccoli', 'chocola', 'vanille', 'ijs', 'chocola', 'ijs', 'chocola', 'wortel', 'zon', 'strand', 'zon', 'strand', 'broccoli', 'strand', 'wortel', 'strand', 'broccoli', 'ijs', 'broccoli', 'ijs', 'ijs', 'vanille', 'strand', 'zon', 'zon', 'strand', 'ijs', 'strand', 'ijs', 'ijs', 'zon', 'wortel', 'ijs', 'ijs', 'vanille', 'broccoli', 'zon', 'ijs', 'chocola', 'chocola', 'zon', 'vanille', 'ijs', 'chocola', 'wortel', 'chocola', 'broccoli', 'zon', 'zon', 'wortel', 'chocola', 'broccoli', 'wortel', 'strand', 'strand', 'strand', 'strand', 'zon', 'vanille', 'vanille', 'ijs', 'wortel', 'chocola', 'vanille', 'zon', 'ijs', 'wortel', 'zon', 'ijs', 'wortel', 'wortel', 'zon', 'chocola', 'ijs', 'chocola', 'strand', 'chocola', 'vanille', 'ijs', 'ijs', 'ijs', 'chocola']\n"
     ]
    }
   ],
   "source": [
    "# generate the corpus\n",
    "corpus = []\n",
    "\n",
    "for m, tm in enumerate(real_theta):\n",
    "    doc = []\n",
    "    for i in range(length):\n",
    "        # sample topic\n",
    "        topic = np.random.multinomial(1, tm)\n",
    "        topic_index = np.where(topic==1)[0][0]\n",
    "        word = np.random.multinomial(1, real_phi[topic_index])\n",
    "        doc.append(vocabulary[np.where(word==1)[0][0]])\n",
    "    corpus.append(doc)\n",
    "    \n",
    "for i, doc in enumerate(corpus):\n",
    "    print 'document', i+1, doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "d = corpora.Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 86): 1, (3, 35): 0, (4, 36): 2, (2, 84): 0, (0, 76): 1, (4, 66): 1, (1, 64): 2, (2, 78): 0, (0, 98): 0, (3, 86): 0, (0, 17): 1, (1, 28): 2, (2, 27): 1, (0, 55): 1, (3, 2): 2, (1, 54): 0, (4, 5): 1, (2, 53): 2, (0, 45): 0, (3, 40): 2, (1, 40): 2, (4, 35): 0, (2, 47): 1, (0, 67): 2, (4, 73): 1, (1, 89): 1, (2, 73): 2, (3, 95): 0, (4, 87): 2, (2, 99): 2, (1, 21): 1, (2, 18): 2, (0, 14): 0, (3, 11): 1, (1, 15): 0, (4, 12): 1, (2, 12): 0, (0, 36): 2, (3, 17): 2, (1, 33): 2, (4, 42): 2, (2, 38): 2, (0, 90): 0, (3, 55): 0, (4, 48): 2, (1, 82): 0, (2, 64): 2, (3, 68): 1, (4, 94): 0, (0, 5): 1, (1, 0): 2, (4, 11): 0, (2, 7): 0, (0, 59): 2, (3, 22): 2, (1, 58): 0, (4, 17): 1, (2, 33): 2, (0, 81): 2, (3, 60): 0, (4, 63): 0, (2, 91): 2, (3, 77): 0, (4, 69): 1, (1, 77): 2, (3, 83): 2, (4, 99): 2, (0, 28): 1, (1, 25): 2, (2, 30): 2, (0, 50): 0, (3, 31): 2, (1, 51): 0, (4, 24): 2, (2, 56): 2, (0, 40): 1, (3, 37): 0, (4, 38): 1, (2, 82): 0, (0, 78): 2, (4, 76): 1, (1, 70): 2, (2, 76): 0, (3, 88): 1, (0, 19): 2, (1, 18): 2, (2, 25): 2, (0, 9): 2, (3, 4): 2, (1, 52): 1, (4, 7): 1, (2, 51): 1, (0, 47): 2, (3, 42): 1, (1, 46): 0, (4, 45): 2, (2, 45): 1, (0, 69): 0, (3, 48): 1, (4, 75): 2, (1, 95): 0, (2, 71): 0, (3, 65): 0, (4, 81): 0, (2, 97): 0, (2, 16): 2, (0, 0): 1, (3, 13): 0, (1, 13): 1, (4, 14): 2, (2, 10): 1, (0, 38): 1, (3, 19): 1, (1, 39): 2, (4, 20): 1, (2, 36): 1, (0, 92): 2, (3, 57): 0, (4, 50): 1, (1, 80): 2, (2, 94): 2, (3, 70): 1, (4, 88): 2, (1, 74): 1, (0, 7): 2, (1, 6): 2, (2, 5): 1, (0, 61): 0, (3, 24): 0, (1, 56): 1, (4, 19): 1, (2, 63): 2, (0, 83): 1, (3, 62): 0, (4, 57): 1, (2, 89): 1, (0, 73): 1, (3, 79): 2, (4, 71): 1, (1, 67): 1, (3, 85): 1, (0, 30): 1, (1, 31): 2, (2, 28): 2, (0, 52): 2, (3, 1): 1, (1, 49): 0, (4, 26): 2, (2, 54): 0, (0, 42): 1, (3, 39): 1, (1, 43): 0, (4, 32): 1, (2, 80): 2, (0, 64): 1, (4, 78): 2, (1, 68): 0, (2, 74): 2, (3, 90): 1, (4, 84): 1, (0, 21): 2, (3, 96): 1, (1, 16): 1, (2, 23): 2, (0, 11): 0, (3, 6): 2, (1, 10): 1, (4, 1): 0, (2, 49): 1, (0, 33): 0, (3, 44): 2, (1, 44): 1, (4, 47): 1, (2, 43): 2, (0, 71): 2, (3, 50): 2, (4, 53): 2, (1, 93): 2, (2, 69): 1, (3, 67): 2, (4, 83): 2, (0, 2): 2, (3, 15): 2, (1, 3): 0, (4, 8): 0, (2, 8): 1, (0, 56): 1, (3, 21): 1, (1, 37): 0, (4, 22): 0, (2, 34): 0, (0, 94): 1, (3, 59): 0, (4, 60): 1, (1, 86): 2, (2, 92): 2, (3, 72): 2, (4, 90): 0, (1, 72): 1, (4, 96): 0, (1, 98): 2, (0, 25): 1, (1, 4): 2, (2, 3): 1, (0, 63): 0, (3, 26): 0, (1, 62): 2, (4, 29): 0, (2, 61): 0, (0, 85): 2, (3, 32): 2, (4, 59): 0, (2, 87): 0, (0, 75): 2, (4, 65): 2, (1, 65): 0, (0, 97): 2, (3, 87): 2, (0, 16): 1, (1, 29): 1, (2, 26): 1, (0, 54): 2, (3, 3): 0, (1, 55): 0, (4, 4): 0, (2, 52): 1, (0, 44): 2, (3, 41): 1, (1, 41): 1, (4, 34): 2, (2, 46): 2, (0, 66): 2, (4, 72): 0, (1, 90): 1, (2, 72): 0, (3, 92): 0, (4, 86): 1, (2, 98): 0, (0, 23): 1, (3, 98): 0, (1, 22): 0, (2, 21): 0, (0, 13): 0, (3, 8): 1, (1, 8): 1, (4, 3): 2, (2, 15): 1, (0, 35): 1, (3, 46): 2, (1, 34): 0, (4, 41): 2, (2, 41): 0, (0, 89): 1, (3, 52): 0, (4, 55): 0, (1, 83): 2, (2, 67): 1, (3, 69): 1, (4, 93): 0, (0, 4): 2, (1, 1): 2, (4, 10): 1, (2, 6): 0, (0, 58): 1, (3, 23): 2, (1, 59): 2, (4, 16): 0, (2, 32): 0, (0, 80): 2, (3, 61): 1, (4, 62): 2, (1, 84): 2, (2, 90): 1, (3, 74): 0, (4, 68): 1, (1, 78): 1, (3, 80): 0, (4, 98): 1, (1, 96): 2, (0, 27): 2, (1, 26): 2, (2, 1): 1, (0, 49): 2, (3, 28): 1, (1, 60): 2, (4, 31): 2, (2, 59): 0, (0, 87): 1, (3, 34): 0, (4, 37): 1, (2, 85): 1, (0, 77): 1, (4, 67): 1, (1, 71): 0, (2, 79): 1, (0, 99): 1, (3, 89): 1, (0, 18): 0, (1, 19): 1, (2, 24): 2, (0, 8): 0, (3, 5): 2, (1, 53): 1, (4, 6): 0, (2, 50): 0, (0, 46): 2, (3, 43): 1, (1, 47): 1, (4, 44): 1, (2, 44): 1, (0, 68): 2, (3, 49): 2, (4, 74): 1, (1, 88): 0, (2, 70): 0, (3, 94): 1, (4, 80): 0, (2, 96): 0, (1, 20): 1, (2, 19): 2, (0, 15): 2, (3, 10): 0, (1, 14): 0, (4, 13): 0, (2, 13): 0, (0, 37): 2, (3, 16): 0, (1, 32): 1, (4, 43): 0, (2, 39): 1, (0, 91): 1, (3, 54): 1, (4, 49): 2, (1, 81): 1, (2, 65): 1, (3, 71): 0, (4, 95): 1, (1, 75): 0, (0, 6): 2, (1, 7): 1, (2, 4): 1, (0, 60): 0, (3, 25): 0, (1, 57): 2, (4, 18): 2, (2, 62): 1, (0, 82): 0, (3, 63): 0, (4, 56): 1, (2, 88): 2, (0, 72): 0, (3, 76): 2, (4, 70): 2, (1, 76): 2, (3, 82): 1, (0, 29): 1, (1, 24): 0, (2, 31): 2, (0, 51): 1, (3, 30): 1, (1, 50): 1, (4, 25): 2, (2, 57): 1, (0, 41): 1, (3, 36): 0, (4, 39): 2, (2, 83): 0, (0, 79): 1, (4, 77): 1, (1, 69): 1, (2, 77): 1, (3, 91): 2, (0, 20): 0, (3, 97): 0, (1, 17): 1, (2, 22): 1, (0, 10): 1, (3, 7): 0, (1, 11): 1, (4, 0): 2, (2, 48): 1, (0, 32): 1, (3, 45): 1, (1, 45): 0, (4, 46): 1, (2, 42): 0, (0, 70): 2, (3, 51): 2, (4, 52): 0, (1, 94): 0, (2, 68): 0, (3, 64): 1, (4, 82): 0, (2, 17): 2, (0, 1): 2, (3, 12): 0, (1, 12): 2, (4, 15): 0, (2, 11): 0, (0, 39): 0, (3, 18): 2, (1, 38): 2, (4, 21): 2, (2, 37): 0, (0, 93): 2, (3, 56): 2, (4, 51): 2, (1, 87): 2, (2, 95): 2, (3, 73): 0, (4, 89): 2, (1, 73): 0, (1, 99): 1, (0, 24): 1, (1, 5): 0, (2, 2): 0, (0, 62): 0, (3, 27): 1, (1, 63): 0, (4, 28): 1, (2, 60): 1, (0, 84): 2, (3, 33): 1, (4, 58): 1, (2, 86): 1, (0, 74): 1, (3, 78): 0, (4, 64): 0, (1, 66): 2, (0, 96): 1, (3, 84): 1, (0, 31): 0, (1, 30): 2, (2, 29): 2, (0, 53): 1, (3, 0): 0, (1, 48): 2, (4, 27): 1, (2, 55): 0, (0, 43): 0, (3, 38): 2, (1, 42): 1, (4, 33): 1, (2, 81): 0, (0, 65): 1, (4, 79): 2, (1, 91): 1, (2, 75): 1, (3, 93): 1, (4, 85): 1, (0, 22): 2, (3, 99): 2, (1, 23): 2, (2, 20): 0, (0, 12): 1, (3, 9): 1, (1, 9): 1, (4, 2): 1, (2, 14): 0, (0, 34): 2, (3, 47): 0, (1, 35): 1, (4, 40): 1, (2, 40): 1, (0, 88): 2, (3, 53): 0, (4, 54): 1, (1, 92): 1, (2, 66): 0, (3, 66): 2, (4, 92): 2, (0, 3): 0, (3, 14): 1, (1, 2): 2, (4, 9): 2, (2, 9): 0, (0, 57): 1, (3, 20): 2, (1, 36): 1, (4, 23): 0, (2, 35): 1, (0, 95): 0, (3, 58): 1, (4, 61): 2, (1, 85): 2, (2, 93): 1, (3, 75): 0, (4, 91): 0, (1, 79): 0, (3, 81): 2, (4, 97): 2, (1, 97): 1, (0, 26): 1, (1, 27): 2, (2, 0): 1, (0, 48): 1, (3, 29): 0, (1, 61): 1, (4, 30): 2, (2, 58): 0}\n",
      "[[23 41 36]\n",
      " [27 36 37]\n",
      " [36 36 28]\n",
      " [37 33 30]\n",
      " [27 38 35]]\n",
      "500\n",
      "[[30 34 22 19 21  9 15]\n",
      " [34 41 19 24 23 20 23]\n",
      " [30 23 17 22 25 20 29]]\n",
      "500\n",
      "Counter({1: 184, 2: 166, 0: 150})\n"
     ]
    }
   ],
   "source": [
    "# initialize z\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "z = {}\n",
    "ndk = np.zeros((len(corpus), len(real_phi)), dtype=np.int)\n",
    "nkw = np.zeros((len(real_phi), len(vocabulary)), dtype=np.int)\n",
    "nk = Counter()\n",
    "\n",
    "for d, doc in enumerate(corpus):\n",
    "    for i, word in enumerate(doc):\n",
    "        t = random.randint(0,2)\n",
    "        z[(d, i)] = t\n",
    "        ndk[d][t] += 1\n",
    "        word_index = np.where(vocabulary==word)[0][0]\n",
    "        nkw[t][word_index] += 1\n",
    "        nk[t] += 1\n",
    "\n",
    "print z\n",
    "print ndk\n",
    "print np.sum(ndk)\n",
    "print nkw\n",
    "print np.sum(nkw)\n",
    "print nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  37  63]\n",
      " [ 53   0  47]\n",
      " [100   0   0]\n",
      " [ 27  73   0]\n",
      " [ 28  48  24]]\n",
      "[[21 54  2 65 58  2  6]\n",
      " [73 44 30  0 11  0  0]\n",
      " [ 0  0 26  0  0 47 61]]\n",
      "Counter({0: 208, 1: 158, 2: 134})\n"
     ]
    }
   ],
   "source": [
    "# iterate\n",
    "\n",
    "def p_z(alpha, beta, num_topics, num_words, d, topic, word_index):\n",
    "    #print alpha, beta, num_topics, num_words, d, topic, word_index\n",
    "    #print ndk[d][topic], nkw[topic][word_index], nk[topic]\n",
    "    return (ndk[d][topic]*alpha)*(nkw[topic][word_index]+beta)/(nk[topic]+beta*num_words)\n",
    "\n",
    "def normalize(p):\n",
    "    minimum = np.min(p)\n",
    "    maximum = np.max(p)\n",
    "    \n",
    "    if minimum < 0:\n",
    "        return normalize((p - minimum)/(maximum - minimum))\n",
    "    return p/sum(p)\n",
    "\n",
    "num_iter = 100\n",
    "\n",
    "alpha = 0.02\n",
    "beta = 0.02\n",
    "\n",
    "theta = np.zeros((num_iter,  len(corpus), num_topics))\n",
    "phi = np.zeros((num_iter, num_topics, len(vocabulary)))\n",
    "\n",
    "for t in range(num_iter):\n",
    "    for d, doc in enumerate(corpus):\n",
    "        for i, w in enumerate(doc):\n",
    "            word = (d, i)\n",
    "            topic = z[word]\n",
    "                        \n",
    "            word_index = np.where(vocabulary==w)[0][0]\n",
    "                \n",
    "            #if ndk[d][topic] > 0 and nkw[topic][word_index] > 0 and nk[topic] > 0:\n",
    "            if True:\n",
    "                ndk[d][topic] -= 1\n",
    "                nkw[topic][word_index] -= 1\n",
    "                nk[topic] -= 1\n",
    "            \n",
    "                p = [p_z(alpha, beta, len(real_phi), len(vocabulary), d, j, word_index) for j in range(len(real_phi))]\n",
    "                #print p\n",
    "                # normalize\n",
    "                p = normalize(p)\n",
    "                #print p, sum(p)\n",
    "            \n",
    "                to = np.random.multinomial(1, p)\n",
    "                topic = np.where(to==1)[0][0]\n",
    "                \n",
    "                z[word] = topic\n",
    "            \n",
    "                ndk[d][topic] += 1\n",
    "                nkw[topic][word_index] += 1\n",
    "                nk[topic] += 1\n",
    "    # calculate theta and phi\n",
    "    theta[t] = (ndk+float(alpha))/(np.sum(ndk, axis=1, keepdims=True)+num_topics*alpha)\n",
    "    phi[t] = (nkw+float(beta))/(np.sum(nkw, axis=1, keepdims=True)+len(vocabulary)*beta)\n",
    "            #print 'new topic', topic\n",
    "print ndk\n",
    "print nkw\n",
    "print nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta\n",
      "found\n",
      "[[ 0.00239856  0.33569858  0.66190286]\n",
      " [ 0.45712572  0.0064961   0.53637817]\n",
      " [ 0.9895063   0.00829502  0.00219868]\n",
      " [ 0.33439936  0.66060364  0.004997  ]\n",
      " [ 0.3971617   0.35978413  0.24305417]]\n",
      "real\n",
      "[[ 0.5  0.   0.5]\n",
      " [ 0.3  0.3  0.4]\n",
      " [ 0.1  0.8  0.1]\n",
      " [ 0.7  0.3  0. ]\n",
      " [ 0.4  0.4  0.2]]\n",
      "\n",
      "phi\n",
      "found\n",
      "[[ 0.0921228   0.24737625  0.01099439  0.29633911  0.30185129  0.01459099\n",
      "   0.03672517]\n",
      " [ 0.43187753  0.29995454  0.24493072  0.00216886  0.00874174  0.00634798\n",
      "   0.00597863]\n",
      " [ 0.09803914  0.01669008  0.15297795  0.00137759  0.01012465  0.31450553\n",
      "   0.40628508]]\n",
      "real\n",
      "[[ 0.4   0.2   0.4   0.    0.    0.    0.  ]\n",
      " [ 0.    0.3   0.    0.35  0.35  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.5   0.5 ]]\n",
      "\n",
      "topics found:\n",
      "['zon' 'ijs' 'strand' 'vanille' 'chocola' 'broccoli' 'wortel']\n",
      "['zon' 'ijs' 'strand']\n",
      "['zon' 'ijs' 'strand' 'chocola' 'broccoli' 'wortel']\n",
      "\n",
      "topics real\n",
      "['zon' 'ijs' 'strand']\n",
      "['ijs' 'vanille' 'chocola']\n",
      "['broccoli' 'wortel']\n"
     ]
    }
   ],
   "source": [
    "print 'theta'\n",
    "print 'found'\n",
    "print np.mean(theta, axis=0)\n",
    "print 'real'\n",
    "print real_theta\n",
    "print\n",
    "print 'phi'\n",
    "print 'found'\n",
    "print np.mean(phi, axis=0)\n",
    "print 'real'\n",
    "print real_phi\n",
    "print\n",
    "print 'topics found:'\n",
    "indexes = np.mean(phi, axis=0) > 0.01\n",
    "for index in indexes:\n",
    "    print vocabulary[index]\n",
    "print\n",
    "print 'topics real'\n",
    "indexes = real_phi > 0.01\n",
    "for index in indexes:\n",
    "    print vocabulary[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
